{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FWI in the Cloud with Devito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "# Set up inversion parameters.\n",
    "param = {'t0': 0.,\n",
    "         'tn': 4000.,              # Simulation last 4 second (4000 ms)\n",
    "         'f0': 0.008,              # Source peak frequency is 5Hz (0.005 kHz)\n",
    "         'nshots': 97**2,          # Number of shots to create gradient from\n",
    "         'm_bounds': (0.08, 0.25), # Set the min and max slowness\n",
    "         'nbpml': 40}              # nbpml thickness.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy import signal, optimize\n",
    "\n",
    "from devito import Grid\n",
    "\n",
    "from distributed import Client, LocalCluster, wait\n",
    "\n",
    "import cloudpickle as pickle\n",
    "\n",
    "# Import acoustic solver, source and receiver modules.\n",
    "from examples.seismic import Model, demo_model\n",
    "from examples.seismic.acoustic import AcousticWaveSolver\n",
    "from examples.seismic import TimeAxis, PointSource, RickerSource, Receiver\n",
    "\n",
    "# Import convenience function for plotting results\n",
    "from examples.seismic import plot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/Users/ggorman/Downloads/seg-demo-project-2-2fa1703fb1c7.json\"\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/home/jovyan/seg-demo-project-2-2fa1703fb1c7.json\"\n",
    "\n",
    "def download_file_from_bucket(filename):    \n",
    "    client = storage.Client(project='seg-demo-project-2')\n",
    "    bucket = client.get_bucket('datasets-proxy')\n",
    "    blob = bucket.get_blob(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        blob.download_to_file(f)\n",
    "\n",
    "def upload_file_to_bucket(filename):\n",
    "    client = storage.Client(project='seg-demo-project-2')\n",
    "    bucket = client.get_bucket('datasets-proxy')\n",
    "    blob = storage.Blob(filename, bucket)\n",
    "    with open(filename, 'rb') as f:\n",
    "        blob.upload_from_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def from_hdf5(filename, **kwargs):\n",
    "    print(\"Open file\")\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    print(\"Read origin\")\n",
    "    origin = kwargs.pop('origin', None)\n",
    "    if origin is None:\n",
    "        origin_key = kwargs.pop('origin_key', 'o')\n",
    "        origin = tuple(f[origin_key][()])\n",
    "\n",
    "    print(\"Read spacing\")\n",
    "    spacing = kwargs.pop('spacing', None)\n",
    "    if spacing is None:\n",
    "        spacing_key = kwargs.pop('spacing_key', 'd')\n",
    "        spacing = tuple(f[spacing_key][()])\n",
    "    \n",
    "    print(\"Read nbpml\")\n",
    "    nbpml = kwargs.pop('nbpml', 20)\n",
    "    datakey = kwargs.pop('datakey', None)\n",
    "    if datakey is None:\n",
    "        raise ValueError(\"datakey must be known - what is the name of the data in the file?\")\n",
    "    \n",
    "    print(\"Read space_order\")\n",
    "    space_order=kwargs.pop('space_order', None)\n",
    "    dtype = kwargs.pop('dtype', None)\n",
    "    data_m = f[datakey][()]\n",
    "    data_vp = np.sqrt(1/data_m).astype(dtype)\n",
    "    data_vp = np.transpose(data_vp, (1, 2, 0))\n",
    "    shape = data_vp.shape\n",
    "    \n",
    "    print(\"Close the file\")\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Instantiate Model\")\n",
    "    \n",
    "    return Model(space_order=space_order, vp=data_vp, origin=origin, shape=shape,\n",
    "                 dtype=dtype, spacing=spacing, nbpml=nbpml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_model():\n",
    "    filename = 'overthrust_3D_true_model.h5'\n",
    "    \n",
    "    model_file = Path(filename)\n",
    "    if not model_file.is_file():\n",
    "        download_file_from_bucket(filename)\n",
    "        \n",
    "    return from_hdf5(filename, nbpml=param['nbpml'], space_order=4,\n",
    "                     datakey='m', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_shot_data(shot_id, src, rec):\n",
    "    ''' Dump shot data to disk.\n",
    "    '''\n",
    "    filename = 'shot_%d.p'%shot_id\n",
    "    pickle.dump({'src':src, 'rec':rec}, open(filename, \"wb\"))\n",
    "\n",
    "    upload_file_to_bucket(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shotdata_i(param):\n",
    "    \"\"\" Inversion crime alert! Here the worker is creating the\n",
    "        'observed' data using the real model. For a real case\n",
    "        the worker would be reading seismic data from disk.\n",
    "    \"\"\"\n",
    "    log = open(\"log_%d.txt\", 'w')\n",
    "    \n",
    "    log.write(\"get_true_model()\")\n",
    "    \n",
    "    true_model = get_true_model()\n",
    "    shot_id = param['shot_id']\n",
    "    \n",
    "    param['shape'] = true_model.vp.shape\n",
    "    param['spacing'] = true_model.spacing\n",
    "    param['origin'] = true_model.origin\n",
    "\n",
    "    i = shot_id%97\n",
    "    j = int(shot_id/97)\n",
    "    \n",
    "    # Time step from model grid spacing\n",
    "    dt = true_model.critical_dt\n",
    "\n",
    "    # Set up source data and geometry.\n",
    "    log.write(\"Set up source data and geometry\")\n",
    "    time_range = TimeAxis(start=param['t0'], stop=param['tn'], step=dt)\n",
    "    src = RickerSource(name='src', grid=true_model.grid, f0=param['f0'],\n",
    "                       time_range=time_range)\n",
    "\n",
    "    src.coordinates.data[0, :] = [400+i*4*true_model.spacing[0], 400+j*4*true_model.spacing[1], 50] \n",
    "    \n",
    "    # Number of receiver locations per shot.\n",
    "    nreceivers = 97**2\n",
    "\n",
    "    # Set up receiver data and geometry.\n",
    "    log.write(\"Set up receiver data and geometry.\")\n",
    "    rec = Receiver(name='rec', grid=true_model.grid, time_range=time_range,\n",
    "                   npoint=nreceivers)\n",
    "    \n",
    "    for n in range(97):\n",
    "        for m in range(97):\n",
    "            rec.coordinates.data[:, 0] = 400+n*4*true_model.spacing[0]\n",
    "            rec.coordinates.data[:, 1] = 400+m*4*true_model.spacing[1]\n",
    "            rec.coordinates.data[:, 2] = 50\n",
    "\n",
    "    # Set up solver.\n",
    "    log.write(\"Set up solver.\")\n",
    "    solver = AcousticWaveSolver(true_model, src, rec, space_order=4)\n",
    "\n",
    "    # Generate synthetic receiver data from true model.\n",
    "    log.write(\"Generate synthetic receiver data from true model.\")\n",
    "    true_d, _, _ = solver.forward(src=src, m=true_model.m)\n",
    "\n",
    "    log.write(\"dump_shot_data\")\n",
    "    dump_shot_data(shot_id, src, true_d)\n",
    "    \n",
    "    log.close()\n",
    "\n",
    "def generate_shotdata(param):\n",
    "    # Define work list\n",
    "    work = [dict(param) for i in range(param['nshots'])]\n",
    "    for i in  range(param['nshots']):\n",
    "        work[i]['shot_id'] = i\n",
    "        \n",
    "    # Map worklist to cluster\n",
    "    futures = client.map(generate_shotdata_i, work)\n",
    "        \n",
    "    # Wait for all futures\n",
    "    wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Dask cluster\n",
    "cluster = LocalCluster(n_workers=1, death_timeout=600)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Generate shot data.\n",
    "generate_shotdata(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
